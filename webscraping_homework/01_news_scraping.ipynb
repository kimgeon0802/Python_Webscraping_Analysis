{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa9b827",
   "metadata": {},
   "source": [
    "#### 1-1. Daum 뉴스기사 제목 스크래핑하기 \n",
    "질문1. 뉴스기사의 링크와 제목을 출력하세요.\n",
    "다음 경제 뉴스 URL\n",
    "url = 'https://news.daum.net/economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req_param = {\n",
    "    'sid': 'economy'\n",
    "}\n",
    "url = 'https://news.daum.net/{sid}'.format(**req_param)\n",
    "\n",
    "req_header = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.3'\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=req_header)\n",
    "res.encoding = 'utf-8'\n",
    "if res.ok:\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    #print(soup)\n",
    "    for li_tag in soup.select('ul.list_newsheadline2 li'):\n",
    "        a_tag = li_tag.find('a')\n",
    "        link = a_tag['href']\n",
    "            \n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "        title = strong_tag.text\n",
    "\n",
    "        print(title, link)\n",
    "else:\n",
    "    print(f'Error Code={res.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af2969",
   "metadata": {},
   "source": [
    "질문2. 여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87998cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req_header = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.3'\n",
    "}\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics','국제':'world','문화':'culture','생활':'life','IT/과학':'tech','인물':'people'}\n",
    "\n",
    "def print_news(section_name):\n",
    "    if len(section_name) >= 2: #입력한 받은 문자의 길이가 2이상일 경우에만 변환\n",
    "        for i in range(0,len(section_dict)): #Section_Dict의 길이 만큼 반복\n",
    "            if section_name in list(section_dict.keys())[i]: #입력한 문자가 DICT에 포함되어 있는지 확인\n",
    "                section_name = list(section_dict.keys())[i] # 입력한 문자가 DICT에 포함되어 있는 문자라면 DICT의 키 값으로 변환\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        section_name = \"정치\"\n",
    "    \n",
    "    sid = section_dict.get(section_name,'정치')\n",
    "    url = f'https://news.daum.net/{sid}'\n",
    "    print(f'{section_name} 뉴스 {url}')\n",
    "    req_header = {\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.3'\n",
    "    }\n",
    "    res = requests.get(url, headers=req_header)\n",
    "    res.encoding = 'utf-8'\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        for li_tag in soup.select('ul.list_newsheadline2 li'):\n",
    "            a_tag = li_tag.find('a')\n",
    "            link = a_tag['href']\n",
    "            \n",
    "            strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "            title = strong_tag.text\n",
    "\n",
    "            print(title, link)\n",
    "    else:\n",
    "        print(f'Error Code={res.status_code}')\n",
    "\n",
    "print_news(\"I\")\n",
    "#print(len(section_dict.keys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
